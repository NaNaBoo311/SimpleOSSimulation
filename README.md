## Goal
The objective of this project is the simulation of major components in a simple operating system,
for example, scheduler, synchronization, related operations of physical memory and virtual memory.

## Scheduler
This operating system is designed to support **multi-processor environments**. It employs a **multilevel ready queue** structure to determine which process should be executed when a CPU becomes available. Each queue is assigned a fixed **priority level**, and the scheduler uses a **multilevel queue scheduling algorithm**, similar to the one implemented in the **Linux kernel**. This approach ensures efficient CPU utilization and organized task prioritization across multiple processors.

For each new program, the loader will create a new process and assign a new PCB to it. The loader then reads and copies the content of the program to the text segment of the new process.
Each process in the system has an associated **priority value (`prio`)**. The **Process Control Block (PCB)** of the process is pushed into the corresponding **ready queue** that matches its priority level. The process then waits to be scheduled on the CPU.

The CPU executes processes using a **round-robin** approach, allowing each process to run for a fixed **time slice**. After its time slice expires, the process is re-enqueued into its associated priority queue, and the CPU selects the next available process from the ready queues.

This system implements the **Multi-Level Queue (MLQ)** scheduling policy with `MAX_PRIO` distinct priority levels. Unlike the Linux kernel, which may group priorities into subsets, this design maintains **one queue per priority level** for simplicity.

- `add_proc`: Places a process into the appropriate ready queue based on its priority.
- `get_proc`: Retrieves the next process to dispatch, following MLQ rules.
- The CPU dispatch mechanism uses `get_proc` to select and schedule processes in order of priority and readiness.

This architecture ensures clear priority-based scheduling and simulates MLQ behavior in a simplified yet effective manner.
The description of MLQ policy: the traversed step of ready queue list is a fixed formulated number based on the priority, i.e. slot= (MAX PRIO - prio), each queue have only fixed slot to use the CPU and when it is used up, the system must change the resource to the other process in the next queue and left the remaining work for future slot even though it needs a completed round of ready queue.

## Memory Management
### Virtual memory in each process
The virtual memory space is organized as a memory mapping for each process PCB. From the process point of view, the virtual address includes multiple vm areas (contiguously). In the real world, each area can act as code, stack or heap segment. Therefore, the process keeps in its pcb a pointer of multiple contiguous memory areas.
Each memory area spans a continuous range between `[vm_start, vm_end]`. Although this entire space is defined, the **actual usable area** is restricted by the top pointer, which points to `sbrk`.

Within the region between `vm_start` and `sbrk`, memory is organized into:

- **Allocated regions**: Tracked by `struct vm_rg_struct`
- **Free slots**: Managed via the `vm_freerg_list`

This design ensures that **physical memory allocation** occurs only within the defined **usable area**, allowing efficient and manageable virtual memory usage.

**Memory region** are actually acted as the variables in the human-readable program’s source code. Due to the current out-of-scope fact, I simply touch in the concept of namespace in term of indexing. I has not been equipped enough the principle of the
compiler. It is, again, overwhelmed to employs such a complex symbol table in this simulation. I temporarily imagine these regions as a set of limited-sized memory regions. These are managed using an array `symrgtbl[PAGING MAX SYMTBL SZ]`, the array size is fixed by a constant, `PAGING MAX SYMTBL SZ`, denoted the number of variable allowed in each program. To wrap up, I use the `struct vm_rg_struct_symrgtbl` to keep the start and the end point of the region and the pointer `rg_next` is reserved for future set tracking

**Memory mapping** is represented by `struct mm_struct`, which tracks all the mentioned memory regions in a separated contiguous memory area. In each memory mapping struct, many memory areas are pointed out by `struct vm_area_struct *mmap list`. An important field is the `pgd`, which is the page table directory and contains all page table entries. Each entry maps the page number to the frame number in the paging memory management system. The `symrgtbl` is a simple implementation of the symbol table. The other fields are mainly used to track specific user operations i.e. caller, fifo page (for referencing).

**CPU addresses** the address generated by CPU to access a specific memory location. In paging-based system, it is divided into:
- **Page number (p)**: used as an index into a page table that holds the based address for each page in physical memory.
- **Page offset (d)**: combined with base address to define the physical memory address that is sent to the Memory Management Unit

In the VM summary, all structures supporting VM are placed in the module **mm-vm.c**

### The system’s physical memory
All processes own their separated memory mappings, but all mappings target a singleton physical device. There are two types of devices, RAM and SWAP. They both can be implemented by the same physical device as in **mm-memphy.c** with different settings. The supported settings are random memory access, sequential/serial memory access, and storage capacity.

Despite the various possible configurations, the logical use of these devices can be distinguished. The RAM device, which belongs to the primary memory subsystem, can be accessed directly from the CPU address bus, allowing it to be read or written using CPU instructions. Meanwhile, SWAP is just a secondary memory device, and all data manipulation must be performed by moving them to the main memory. Since it lacks direct access from the CPU, the system usually equips a large SWAP at a low cost and may have more than one instance. In my settings, I support hardware configured with one RAM device and up to 4 SWAP devices.

The struct `framephy_struct` is mainly used to store the frame number.

The struct `memphy_struct` has basic fields storage and size. The `rdmflg` field defines whether the
memory access is random or sequential. The fields `free_fp_list` and used `fp_list` are reserved for
retaining unused and used memory frames, respectively.

### Paging-based address translation scheme
The translation supports both segmentation and segmentation with paging. In this version, I develop
a single-level paging system that leverages one RAM device and one SWAP instance hardware. I have
implemented the capability to handle multiple memory segments, but I mainly focus on the first segment
of `vm_area` (vmaid = 0). The further versions will take into account a sufficient paging scheme for multiple
segments and the potential overlap/non-overlap between segments.

![Page Table Entry Format.](https://scontent.fsgn5-14.fna.fbcdn.net/v/t1.15752-9/494358102_2090049134848014_7076033374834103086_n.png?_nc_cat=101&ccb=1-7&_nc_sid=9f807c&_nc_ohc=pmwKkhqh6SwQ7kNvwGuYRMM&_nc_oc=AdkhY3yAJY4SZBw-wqQRasfNcXJDXW5o1u1D9Fbkn9ecB7NkJ2V0sg89WUEITNS9P-g&_nc_zt=23&_nc_ht=scontent.fsgn5-14.fna&oh=03_Q7cD2QG92lF9wIfey3ISoxtz3BIe3LuBtxZ3eeTDXEcchWu48w&oe=6858F868)
This structure allows a userspace process to determine which physical frame each virtual page is mapped to.
It contains a 32-bit value for each virtual page, containing the following data:
- **If Present:**
  - Bits 0–12: Page Frame Number (FPN)
  - Bits 13–14: Zero
  - Bits 15–27: User-defined numbering

- **If Swapped:**
  - Bits 0–4: Swap Type
  - Bits 5–25: Swap Offset

- **Other Flags:**
  - Bit 28: Dirty
  - Bit 29: Reserved
  - Bit 30: Swapped
  - Bit 31: Present

**Page table** The virtual space is isolated for each entity, so each `struct pcb_t` has its own table. In all cases, each process has a completely isolated and unique space, N processes in
our setting result in N page tables. Each page must have all entries for the entire CPU address space. For
each entry, the paging number may have an associated frame in MEMRAM or MEMSWP, or might have null value. 

the process can access the virtual memory space in a contiguous manner of the `vm_area`
structure. The remaining work deals with the mapping between page and frame to provide the contiguous
memory space over the discrete frame storing mechanism. This falls into two main approaches: memory
swapping and basic memory operations, i.e. alloc/free/read/write, which mostly keep in touch with pgd
page table structure.

`Memory swapping` I have been informed that a memory area (/segment) may not be used up to its
limit storage space. It means that some storage spaces remain unmapped to MEMRAM. Swapping can help moving the contents of physical frame between the MEMRAM and MEMSWAP. The swapping is a
mechanism that copies the frame’s content from outside to main memory (RAM). Swapping out, in reverse,
attempts to move the content of a frame in MEMRAM to MEMSWAP. In a typical context, swapping helps
free up frame of RAM since the size of SWAP device is usually large enough.

**Basic memory operations in paging-based system**
- **ALLOC**  
  - The user calls library functions in `libmem`, which typically allocate space within the data segment.
  - If no suitable space is available, I expand the memory by lifting the barrier set by `sbrk`.
  - Since the new space may not have been previously used, I leverage MMU system calls to obtain physical frames and map them using Page Table Entries (PTE).

- **FREE**  
  - The user calls `libmem` functions to release the memory region associated with a given region ID.
  - As reclaiming physical frames could lead to memory holes, I do not free them immediately.
  - Instead, I store them in a free list for future allocation requests, all handled within the `libmem` library.

- **READ/WRITE**  
  - These operations require the corresponding page to be present in main memory.
  - The most resource-intensive step is page swapping:
    - If the page is in the `MEMSWAP` device, I bring it back to the `MEMRAM` device (swapping in).
    - If there is insufficient space in `MEMRAM`, I swap out other pages to `MEMSWAP` to make room.
